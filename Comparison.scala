import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import  org.apache.log4j.{Level,Logger}

object Comparison {
  def main(args: Array[String]): Unit = {
     System.setProperty("hadoop.home.dir","D:\\Project_softwares\\Hadoop")//("hadoop.home.dir",Â "D:\\Project_softwares\\Hadoop")
    Logger.getLogger("org").setLevel(Level.ERROR)
    val sp = SparkSession.builder.master("local").appName("Crate DataFrame").getOrCreate()
    import sp.sqlContext.implicits._
    val df = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\PROD DATA\\S_PROD_INT_1.xml")
    //df.printSchema()
    val ProdData1 = df.select(explode($"COL").as("COL"))
      .select($"COL._name",$"COL._VALUE")
    println("ProdData Count",ProdData1.count())
    //ProdData1.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\ProdData1.csv")
   // ProdData1.printSchema()
    ProdData1.show()
    val df1 = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\QA DATA\\QA DATA\\2158268866031333.xml")
        df1.printSchema()
    val QAData1 = df1.select(explode($"COL").as("COL1"))
      .select($"COL1._name",$"COL1._VALUE")
    println("QAData_Insert Count",QAData1.count())
    //QAData1.printSchema()
    QAData1.show()
    //QAData1.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\QAData1.csv")
   // val FinData=(QAData1).except(ProdData1)
    //FinData.coalesce(1).write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\PQData1.csv")
//FinData.show()

    val df2 = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\PROD DATA\\S_PROD_INT_TNTX.xml")
    df2.printSchema()
    val ProdData2 = df2.select(explode($"COL").as("COL2"))
      .select($"COL2._name")
    println("ProdData Count",ProdData2.count())
    //ProdData2.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\ProdData2.csv")
    //ProdData2.printSchema()
    ProdData2.show()
    val df3 = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\QA DATA\\QA DATA\\2158268910986230.xml")
    //df3.printSchema()
    val QAData2 = df3.select(explode($"COL").as("COL3"))
      .select($"COL3._name")
    println("QAData_Insert Count",QAData2.count())
    //QAData2.printSchema()
    QAData2.show()
    //QAData2.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\QAData2.csv")
    val FinData=(QAData2).except(ProdData2)
    FinData.show()
    //val FinData1=(ProdData2.except(QAData2.intersect(ProdData2))).union((QAData2).except(ProdData2.intersect(QAData2)))
    //FinData1.coalesce(1).write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\PQData2.csv")


    //3
    val df4 = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\PROD DATA\\S_PROD_INT_X.xml")
    //df4.printSchema()
    val ProdData3 = df4.select(explode($"COL").as("COL4"))
      .select($"COL4._name",$"COL4._VALUE")
    println("ProdData Count",ProdData3.count())
    //ProdData3.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\ProdData3.csv")
    //ProdData3.printSchema()
    ProdData3.show()
    val df5 = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\QA DATA\\QA DATA\\4739816526017453.xml")
    //df5.printSchema()
    val QAData3= df5.select(explode($"COL").as("COL5"))
      .select($"COL5._name",$"COL5._VALUE")
    println("QAData_Insert Count",QAData3.count())
    //QAData3.printSchema()
    QAData3.show()
   // QAData3.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\QAData3.csv")
    //val FinData2=(ProdData3.except(QAData3.intersect(ProdData3))).union((QAData3).except(ProdData3.intersect(QAData3)))
    //FinData2.coalesce(1).write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\PQData3.csv")

    //4
    val df6 = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\PROD DATA\\S_PROD_REL.xml")
    df6.printSchema()
    val ProdData4 = df6.select(explode($"COL").as("COL6"))
      .select($"COL6._name")
    println("ProdData Count",ProdData4.count())
    //ProdData4.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\ProdData4.csv")
    //ProdData4.printSchema()
    ProdData4.show()
    val df7 = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\QA DATA\\QA DATA\\3258303938660890.xml")
    df7.printSchema()
    val QAData4= df7.select(explode($"COL").as("COL7"))
      .select($"COL7._name")
    println("QAData_Insert Count",QAData4.count())
    //QAData4.printSchema()
    QAData4.show()
    //QAData4.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\QAData4.csv")
    val FinData1=(ProdData4).except(QAData4)
    FinData1.show()
    //5

    val df8 = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\PROD DATA\\S_PROD_STYL_TNT.xml")
    df8.printSchema()
    val ProdData5 = df8.select(explode($"COL").as("COL8"))
      .select($"COL8._name")
    println("ProdData Count",ProdData5.count())
    //ProdData5.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\ProdData5.csv")
    //ProdData5.printSchema()
    ProdData5.show()
    val df9 = sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      .load("D:\\ProjDocs\\Spark_project_code\\QA DATA\\QA DATA\\3258289164351544.xml")
    df9.printSchema()
    val QAData5= df9.select(explode($"COL").as("COL9"))
      .select($"COL9._name")
    println("QAData_Insert Count",QAData5.count())
   // QAData5.printSchema()
    QAData5.show()
    //QAData5.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\QAData5.csv")

    val FinData2=(ProdData5).except(QAData5)
    FinData2.show()





    //val df2= sp.read.format("com.databricks.spark.xml").option("header", "true").option("rowTag", "TABLE").option("rootTag", "COL")
      //.load("D:\\ProjDocs\\Test data of Account and property\\Test data of Account and property\\QA data\\P3_Update.xml")
    ///val QAData_Update = df2.select(explode($"COL").as("COL2"))
      //.select($"COL2._name",$"COL2._datatype")
    //println("QAData_Update Count",QAData_Update.count())
    //QAData_Insert.printSchema()
    //QAData_Update.show()
    //QAData_Update.write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Test data of Account and property\\Test data of Account and property\\P3\\QAData_Update.csv")
    //df1.unionAll(df2).except(df1.intersect(df2))
    //val FinData=ProdData.except(QAData)
    //FinData.show()
    //val FinData1=(ProdData1.except(QAData1.intersect(ProdData1))).union((QAData1).except(ProdData1.intersect(QAData1)))
    //val FinData_1=(ProdData.except(QAData_Update.intersect(ProdData))).union((QAData_Update).except(ProdData.intersect(QAData_Update)))


   // FinData.coalesce(1).write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Spark_project_code\\XMLData\\Data_Mismatch.csv")
   // FinData_1.coalesce(1).write.format("com.databricks.spark.csv").save("D:\\ProjDocs\\Test data of Account and property\\Test data of Account and property\\P3\\Data_Update.csv")
    //println("FinData Count",FinData.count())
   // println("FinData_1 Count",FinData_1.count())
    //FinData.show()
    //FinData_1.show()
  }
}